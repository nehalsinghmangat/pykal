{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LQR Control for TurtleBot3: Optimal Point Stabilization\n",
    "\n",
    "This notebook demonstrates **Linear Quadratic Regulator (LQR)** control for point stabilization on the TurtleBot3 differential drive robot. We show how LQR provides optimal state feedback control and compare it with traditional PID control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "### Point Stabilization Challenge\n",
    "\n",
    "Given a differential drive robot like the TurtleBot3, we want to:\n",
    "- Drive the robot to a target position $(x_{goal}, y_{goal})$\n",
    "- Align with a desired orientation $\\theta_{goal}$\n",
    "- Minimize control effort and tracking error\n",
    "- Handle constraints smoothly (velocity limits)\n",
    "\n",
    "LQR provides an optimal solution by:\n",
    "- Minimizing a quadratic cost function balancing tracking error and control effort\n",
    "- Computing optimal state feedback gains\n",
    "- Guaranteeing stability for linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation\n",
    "\n",
    "### Unicycle Dynamics\n",
    "\n",
    "The TurtleBot3 follows unicycle (differential drive) dynamics:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dot{x} &= v \\cos(\\theta) \\\\\n",
    "\\dot{y} &= v \\sin(\\theta) \\\\\n",
    "\\dot{\\theta} &= \\omega\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $(x, y, \\theta)$: Position and orientation\n",
    "- $v$: Linear velocity (control input)\n",
    "- $\\omega$: Angular velocity (control input)\n",
    "\n",
    "### Linearization\n",
    "\n",
    "To apply LQR (a linear control method), we linearize around the equilibrium (goal state):\n",
    "\n",
    "Define error state:\n",
    "$$\n",
    "\\mathbf{e} = \\begin{bmatrix} x - x_{goal} \\\\ y - y_{goal} \\\\ \\theta - \\theta_{goal} \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Linearized dynamics in discrete time:\n",
    "$$\n",
    "\\mathbf{e}_{k+1} = A \\mathbf{e}_k + B \\mathbf{u}_k\n",
    "$$\n",
    "\n",
    "where:\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "1 & 0 & -v_{ref} \\Delta t \\sin(\\theta_{ref}) \\\\\n",
    "0 & 1 & v_{ref} \\Delta t \\cos(\\theta_{ref}) \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}, \\quad\n",
    "B = \\begin{bmatrix}\n",
    "\\Delta t \\cos(\\theta_{ref}) & 0 \\\\\n",
    "\\Delta t \\sin(\\theta_{ref}) & 0 \\\\\n",
    "0 & \\Delta t\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For point stabilization ($v_{ref} = 0$), this simplifies.\n",
    "\n",
    "### LQR Optimization\n",
    "\n",
    "LQR solves:\n",
    "$$\n",
    "\\min_{\\mathbf{u}_0, \\ldots, \\mathbf{u}_\\infty} \\sum_{k=0}^\\infty \\left( \\mathbf{e}_k^T Q \\mathbf{e}_k + \\mathbf{u}_k^T R \\mathbf{u}_k \\right)\n",
    "$$\n",
    "\n",
    "subject to: $\\mathbf{e}_{k+1} = A \\mathbf{e}_k + B \\mathbf{u}_k$\n",
    "\n",
    "The optimal control is:\n",
    "$$\n",
    "\\mathbf{u}_k^* = -K \\mathbf{e}_k\n",
    "$$\n",
    "\n",
    "where $K$ is computed from the Discrete Algebraic Riccati Equation (DARE):\n",
    "$$\n",
    "P = Q + A^T P A - A^T P B (R + B^T P B)^{-1} B^T P A\n",
    "$$\n",
    "$$\n",
    "K = (R + B^T P B)^{-1} B^T P A\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### System Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import solve_discrete_are\n",
    "\n",
    "# Simulation parameters\n",
    "dt = 0.1  # 100 ms control loop\n",
    "T_sim = 20.0  # 20 second simulation\n",
    "N_steps = int(T_sim / dt)\n",
    "\n",
    "# Robot parameters (TurtleBot3 Burger)\n",
    "V_MAX = 0.22  # m/s (max linear velocity)\n",
    "OMEGA_MAX = 2.84  # rad/s (max angular velocity)\n",
    "\n",
    "# Goal states (multiple waypoints)\n",
    "goals = [\n",
    "    np.array([2.0, 0.0, 0.0]),      # Move forward\n",
    "    np.array([2.0, 2.0, np.pi/2]),  # Move up\n",
    "    np.array([0.0, 2.0, np.pi]),    # Move left\n",
    "    np.array([0.0, 0.0, -np.pi/2])  # Return home\n",
    "]\n",
    "\n",
    "print(f\"Simulation parameters:\")\n",
    "print(f\"  Time step: {dt} s\")\n",
    "print(f\"  Duration: {T_sim} s\")\n",
    "print(f\"  Robot velocity limits: v_max={V_MAX} m/s, omega_max={OMEGA_MAX} rad/s\")\n",
    "print(f\"  Waypoints: {len(goals)} goals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearized Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linearized_dynamics(theta_ref, v_ref, dt):\n",
    "    \"\"\"\n",
    "    Compute linearized unicycle dynamics around reference state.\n",
    "    \n",
    "    Args:\n",
    "        theta_ref: Reference orientation (rad)\n",
    "        v_ref: Reference linear velocity (m/s)\n",
    "        dt: Time step (s)\n",
    "    \n",
    "    Returns:\n",
    "        A, B: Linearized system matrices\n",
    "    \"\"\"\n",
    "    # State: [x, y, theta]\n",
    "    # Control: [v, omega]\n",
    "    \n",
    "    A = np.array([\n",
    "        [1, 0, -v_ref * dt * np.sin(theta_ref)],\n",
    "        [0, 1, v_ref * dt * np.cos(theta_ref)],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    B = np.array([\n",
    "        [dt * np.cos(theta_ref), 0],\n",
    "        [dt * np.sin(theta_ref), 0],\n",
    "        [0, dt]\n",
    "    ])\n",
    "    \n",
    "    return A, B\n",
    "\n",
    "# Example: Linearization at theta=0, v=0 (point stabilization)\n",
    "A_0, B_0 = compute_linearized_dynamics(theta_ref=0.0, v_ref=0.1, dt=dt)\n",
    "\n",
    "print(\"Linearized dynamics (theta=0, v=0.1):\")\n",
    "print(f\"A =\\n{A_0}\")\n",
    "print(f\"B =\\n{B_0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LQR Gain Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykal.algorithm_library.controllers.lqr import LQR\n",
    "def compute_lqr_gain(A, B, Q, R):\n",
    "    \"\"\"\n",
    "    Compute LQR gain using DARE.\n",
    "    \n",
    "    Args:\n",
    "        A, B: System matrices\n",
    "        Q: State cost matrix (n x n)\n",
    "        R: Control cost matrix (m x m)\n",
    "    \n",
    "    Returns:\n",
    "        K: LQR gain matrix (m x n)\n",
    "        P: Solution to DARE (n x n)\n",
    "    \"\"\"\n",
    "    # Solve DARE\n",
    "    P = solve_discrete_are(A, B, Q, R)\n",
    "    \n",
    "    # Compute gain\n",
    "    K = np.linalg.solve(R + B.T @ P @ B, B.T @ P @ A)\n",
    "    \n",
    "    return K, P\n",
    "\n",
    "# Cost matrices\n",
    "Q_pos = 10.0   # Position error weight\n",
    "Q_theta = 5.0  # Orientation error weight\n",
    "Q = np.diag([Q_pos, Q_pos, Q_theta])\n",
    "\n",
    "R_v = 1.0      # Linear velocity effort weight\n",
    "R_omega = 1.0  # Angular velocity effort weight\n",
    "R = np.diag([R_v, R_omega])\n",
    "\n",
    "# Compute LQR gain for point stabilization\n",
    "K_lqr, P_lqr = LQR.compute_gain(A_0, B_0, Q, R)\n",
    "\n",
    "print(\"\\nLQR Configuration:\")\n",
    "print(f\"Q = diag([{Q_pos}, {Q_pos}, {Q_theta}]) (position, position, orientation)\")\n",
    "print(f\"R = diag([{R_v}, {R_omega}]) (linear vel, angular vel)\")\n",
    "print(f\"\\nLQR Gain K =\\n{K_lqr}\")\n",
    "print(f\"\\nCost-to-go P =\\n{P_lqr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LQR Controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lqr_control(state, goal, K, v_max, omega_max):\n",
    "    \"\"\"\n",
    "    LQR controller with velocity saturation.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state [x, y, theta]\n",
    "        goal: Goal state [x_g, y_g, theta_g]\n",
    "        K: LQR gain matrix\n",
    "        v_max: Max linear velocity\n",
    "        omega_max: Max angular velocity\n",
    "    \n",
    "    Returns:\n",
    "        Control input [v, omega]\n",
    "    \"\"\"\n",
    "    # Compute error\n",
    "    error = state - goal\n",
    "    \n",
    "    # Normalize angle error to [-pi, pi]\n",
    "    error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "    \n",
    "    # LQR control law: u = -K * e\n",
    "    u = -K @ error\n",
    "    \n",
    "    # Saturate control\n",
    "    v = np.clip(u[0], -v_max, v_max)\n",
    "    omega = np.clip(u[1], -omega_max, omega_max)\n",
    "    \n",
    "    return np.array([v, omega])\n",
    "\n",
    "# Test LQR controller\n",
    "test_state = np.array([0.5, 0.3, 0.2])\n",
    "test_goal = np.array([0.0, 0.0, 0.0])\n",
    "test_control = lqr_control(test_state, test_goal, K_lqr, V_MAX, OMEGA_MAX)\n",
    "\n",
    "print(f\"\\nController Test:\")\n",
    "print(f\"  State: {test_state}\")\n",
    "print(f\"  Goal: {test_goal}\")\n",
    "print(f\"  Control: v={test_control[0]:.3f} m/s, omega={test_control[1]:.3f} rad/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PID Controller for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PIDController:\n",
    "    \"\"\"Simple PID controller for TurtleBot point stabilization.\"\"\"\n",
    "    \n",
    "    def __init__(self, kp_linear, kp_angular, ki_angular=0.0, kd_angular=0.0):\n",
    "        self.kp_linear = kp_linear\n",
    "        self.kp_angular = kp_angular\n",
    "        self.ki_angular = ki_angular\n",
    "        self.kd_angular = kd_angular\n",
    "        \n",
    "        self.integral = 0.0\n",
    "        self.prev_error = 0.0\n",
    "    \n",
    "    def control(self, state, goal, dt, v_max, omega_max):\n",
    "        \"\"\"\n",
    "        PID control for point stabilization.\n",
    "        \n",
    "        Strategy:\n",
    "        1. Turn to face the goal\n",
    "        2. Drive towards goal\n",
    "        3. Final orientation adjustment\n",
    "        \"\"\"\n",
    "        dx = goal[0] - state[0]\n",
    "        dy = goal[1] - state[1]\n",
    "        distance = np.sqrt(dx**2 + dy**2)\n",
    "        \n",
    "        # Desired heading to goal\n",
    "        desired_theta = np.arctan2(dy, dx)\n",
    "        \n",
    "        # Heading error\n",
    "        theta_error = desired_theta - state[2]\n",
    "        theta_error = np.arctan2(np.sin(theta_error), np.cos(theta_error))\n",
    "        \n",
    "        # Final orientation error (when close to goal)\n",
    "        final_theta_error = goal[2] - state[2]\n",
    "        final_theta_error = np.arctan2(np.sin(final_theta_error), np.cos(final_theta_error))\n",
    "        \n",
    "        # PID terms for angular control\n",
    "        self.integral += theta_error * dt\n",
    "        derivative = (theta_error - self.prev_error) / dt if dt > 0 else 0.0\n",
    "        self.prev_error = theta_error\n",
    "        \n",
    "        # Control logic\n",
    "        if distance > 0.1:  # Far from goal - drive towards it\n",
    "            v = self.kp_linear * distance\n",
    "            omega = self.kp_angular * theta_error + \\\n",
    "                    self.ki_angular * self.integral + \\\n",
    "                    self.kd_angular * derivative\n",
    "        else:  # Close to goal - adjust final orientation\n",
    "            v = 0.0\n",
    "            omega = self.kp_angular * final_theta_error\n",
    "        \n",
    "        # Saturate\n",
    "        v = np.clip(v, -v_max, v_max)\n",
    "        omega = np.clip(omega, -omega_max, omega_max)\n",
    "        \n",
    "        return np.array([v, omega])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset integrator.\"\"\"\n",
    "        self.integral = 0.0\n",
    "        self.prev_error = 0.0\n",
    "\n",
    "# Create PID controller\n",
    "pid_controller = PIDController(\n",
    "    kp_linear=0.5,\n",
    "    kp_angular=2.0,\n",
    "    ki_angular=0.1,\n",
    "    kd_angular=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nPID Controller Created:\")\n",
    "print(f\"  kp_linear: {pid_controller.kp_linear}\")\n",
    "print(f\"  kp_angular: {pid_controller.kp_angular}\")\n",
    "print(f\"  ki_angular: {pid_controller.ki_angular}\")\n",
    "print(f\"  kd_angular: {pid_controller.kd_angular}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicycle Dynamics Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_unicycle(state, control, dt, noise_std=0.01):\n",
    "    \"\"\"\n",
    "    Simulate unicycle dynamics.\n",
    "    \n",
    "    Args:\n",
    "        state: Current state [x, y, theta]\n",
    "        control: Control input [v, omega]\n",
    "        dt: Time step\n",
    "        noise_std: Process noise standard deviation\n",
    "    \n",
    "    Returns:\n",
    "        Next state [x, y, theta]\n",
    "    \"\"\"\n",
    "    x, y, theta = state\n",
    "    v, omega = control\n",
    "    \n",
    "    # Nonlinear dynamics\n",
    "    x_next = x + v * np.cos(theta) * dt\n",
    "    y_next = y + v * np.sin(theta) * dt\n",
    "    theta_next = theta + omega * dt\n",
    "    \n",
    "    # Add process noise\n",
    "    noise = np.random.randn(3) * noise_std\n",
    "    \n",
    "    # Normalize angle\n",
    "    theta_next = np.arctan2(np.sin(theta_next), np.cos(theta_next))\n",
    "    \n",
    "    return np.array([x_next, y_next, theta_next]) + noise\n",
    "\n",
    "print(\"Unicycle dynamics simulator ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation and Results\n",
    "\n",
    "### Multi-Waypoint Navigation with LQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "x_lqr = [np.array([0.0, 0.0, 0.0])]  # Start at origin\n",
    "u_lqr = []\n",
    "\n",
    "current_goal_idx = 0\n",
    "goal_threshold = 0.05  # Switch to next goal when within 5cm\n",
    "\n",
    "print(\"Running LQR simulation...\")\n",
    "for k in range(N_steps):\n",
    "    state = x_lqr[-1]\n",
    "    goal = goals[current_goal_idx]\n",
    "    \n",
    "    # Check if goal reached\n",
    "    error = state - goal\n",
    "    error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "    distance = np.linalg.norm(error[:2])\n",
    "    \n",
    "    if distance < goal_threshold and np.abs(error[2]) < 0.1:\n",
    "        if current_goal_idx < len(goals) - 1:\n",
    "            current_goal_idx += 1\n",
    "            print(f\"  Goal {current_goal_idx} reached at t={k*dt:.1f}s, switching to goal {current_goal_idx+1}\")\n",
    "    \n",
    "    # Compute control\n",
    "    control = lqr_control(state, goals[current_goal_idx], K_lqr, V_MAX, OMEGA_MAX)\n",
    "    u_lqr.append(control)\n",
    "    \n",
    "    # Simulate\n",
    "    next_state = simulate_unicycle(state, control, dt)\n",
    "    x_lqr.append(next_state)\n",
    "\n",
    "x_lqr = np.array(x_lqr[:-1])  # Remove last state (no control for it)\n",
    "u_lqr = np.array(u_lqr)\n",
    "\n",
    "print(f\"LQR simulation complete. Final position: [{x_lqr[-1, 0]:.3f}, {x_lqr[-1, 1]:.3f}, {x_lqr[-1, 2]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Waypoint Navigation with PID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "x_pid = [np.array([0.0, 0.0, 0.0])]  # Start at origin\n",
    "u_pid = []\n",
    "pid_controller.reset()\n",
    "\n",
    "current_goal_idx = 0\n",
    "\n",
    "print(\"Running PID simulation...\")\n",
    "for k in range(N_steps):\n",
    "    state = x_pid[-1]\n",
    "    goal = goals[current_goal_idx]\n",
    "    \n",
    "    # Check if goal reached\n",
    "    error = state - goal\n",
    "    error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "    distance = np.linalg.norm(error[:2])\n",
    "    \n",
    "    if distance < goal_threshold and np.abs(error[2]) < 0.1:\n",
    "        if current_goal_idx < len(goals) - 1:\n",
    "            current_goal_idx += 1\n",
    "            pid_controller.reset()  # Reset integrator for new goal\n",
    "            print(f\"  Goal {current_goal_idx} reached at t={k*dt:.1f}s, switching to goal {current_goal_idx+1}\")\n",
    "    \n",
    "    # Compute control\n",
    "    control = pid_controller.control(state, goals[current_goal_idx], dt, V_MAX, OMEGA_MAX)\n",
    "    u_pid.append(control)\n",
    "    \n",
    "    # Simulate\n",
    "    next_state = simulate_unicycle(state, control, dt)\n",
    "    x_pid.append(next_state)\n",
    "\n",
    "x_pid = np.array(x_pid[:-1])  # Remove last state\n",
    "u_pid = np.array(u_pid)\n",
    "\n",
    "print(f\"PID simulation complete. Final position: [{x_pid[-1, 0]:.3f}, {x_pid[-1, 1]:.3f}, {x_pid[-1, 2]:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Trajectory Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# XY Trajectory\n",
    "ax = axes[0]\n",
    "ax.plot(x_lqr[:, 0], x_lqr[:, 1], 'b-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax.plot(x_pid[:, 0], x_pid[:, 1], 'r-', linewidth=2, label='PID', alpha=0.8)\n",
    "\n",
    "# Plot goals\n",
    "for i, goal in enumerate(goals):\n",
    "    ax.scatter(goal[0], goal[1], c='green', s=200, marker='*', \n",
    "              edgecolors='black', linewidth=2, zorder=5, label='Goals' if i == 0 else '')\n",
    "    ax.annotate(f'G{i+1}', (goal[0], goal[1]), xytext=(5, 5), \n",
    "               textcoords='offset points', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Start point\n",
    "ax.scatter(0, 0, c='blue', s=100, marker='o', label='Start', zorder=5)\n",
    "\n",
    "# Draw orientation arrows at intervals\n",
    "arrow_interval = 20\n",
    "arrow_scale = 0.15\n",
    "for i in range(0, len(x_lqr), arrow_interval):\n",
    "    ax.arrow(x_lqr[i, 0], x_lqr[i, 1], \n",
    "            arrow_scale * np.cos(x_lqr[i, 2]), \n",
    "            arrow_scale * np.sin(x_lqr[i, 2]),\n",
    "            head_width=0.05, head_length=0.05, fc='blue', ec='blue', alpha=0.5)\n",
    "\n",
    "for i in range(0, len(x_pid), arrow_interval):\n",
    "    ax.arrow(x_pid[i, 0], x_pid[i, 1], \n",
    "            arrow_scale * np.cos(x_pid[i, 2]), \n",
    "            arrow_scale * np.sin(x_pid[i, 2]),\n",
    "            head_width=0.05, head_length=0.05, fc='red', ec='red', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('X (m)', fontsize=12)\n",
    "ax.set_ylabel('Y (m)', fontsize=12)\n",
    "ax.set_title('Multi-Waypoint Navigation', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axis('equal')\n",
    "\n",
    "# Time series: Position error\n",
    "ax = axes[1]\n",
    "t_vec = np.arange(len(x_lqr)) * dt\n",
    "\n",
    "# Compute position error to current goal\n",
    "def compute_position_error(states, goals_list, threshold=0.05):\n",
    "    errors = []\n",
    "    goal_idx = 0\n",
    "    for state in states:\n",
    "        goal = goals_list[goal_idx]\n",
    "        error = state - goal\n",
    "        error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "        distance = np.linalg.norm(error[:2])\n",
    "        errors.append(distance)\n",
    "        \n",
    "        # Check goal switch\n",
    "        if distance < threshold and np.abs(error[2]) < 0.1 and goal_idx < len(goals_list) - 1:\n",
    "            goal_idx += 1\n",
    "    return np.array(errors)\n",
    "\n",
    "error_lqr = compute_position_error(x_lqr, goals)\n",
    "error_pid = compute_position_error(x_pid, goals)\n",
    "\n",
    "ax.plot(t_vec, error_lqr, 'b-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax.plot(t_vec, error_pid, 'r-', linewidth=2, label='PID', alpha=0.8)\n",
    "ax.axhline(goal_threshold, color='gray', linestyle='--', alpha=0.5, label='Goal threshold')\n",
    "ax.set_xlabel('Time (s)', fontsize=12)\n",
    "ax.set_ylabel('Position Error (m)', fontsize=12)\n",
    "ax.set_title('Tracking Error Over Time', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control Input Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "t_vec = np.arange(len(u_lqr)) * dt\n",
    "\n",
    "# Linear velocity\n",
    "ax = axes[0]\n",
    "ax.plot(t_vec, u_lqr[:, 0], 'b-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax.plot(t_vec, u_pid[:, 0], 'r-', linewidth=2, label='PID', alpha=0.8)\n",
    "ax.axhline(V_MAX, color='gray', linestyle='--', alpha=0.5, label='Limit')\n",
    "ax.axhline(-V_MAX, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_ylabel('Linear Velocity (m/s)', fontsize=12)\n",
    "ax.set_title('Control Inputs Comparison', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Angular velocity\n",
    "ax = axes[1]\n",
    "ax.plot(t_vec, u_lqr[:, 1], 'b-', linewidth=2, label='LQR', alpha=0.8)\n",
    "ax.plot(t_vec, u_pid[:, 1], 'r-', linewidth=2, label='PID', alpha=0.8)\n",
    "ax.axhline(OMEGA_MAX, color='gray', linestyle='--', alpha=0.5, label='Limit')\n",
    "ax.axhline(-OMEGA_MAX, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Time (s)', fontsize=12)\n",
    "ax.set_ylabel('Angular Velocity (rad/s)', fontsize=12)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\n=== Control Effort Statistics ===\")\n",
    "print(\"\\nLQR:\")\n",
    "print(f\"  Linear velocity: mean={np.mean(np.abs(u_lqr[:, 0])):.3f}, max={np.max(np.abs(u_lqr[:, 0])):.3f} m/s\")\n",
    "print(f\"  Angular velocity: mean={np.mean(np.abs(u_lqr[:, 1])):.3f}, max={np.max(np.abs(u_lqr[:, 1])):.3f} rad/s\")\n",
    "print(f\"  Total control effort: {np.sum(u_lqr[:, 0]**2 + u_lqr[:, 1]**2):.2f}\")\n",
    "\n",
    "print(\"\\nPID:\")\n",
    "print(f\"  Linear velocity: mean={np.mean(np.abs(u_pid[:, 0])):.3f}, max={np.max(np.abs(u_pid[:, 0])):.3f} m/s\")\n",
    "print(f\"  Angular velocity: mean={np.mean(np.abs(u_pid[:, 1])):.3f}, max={np.max(np.abs(u_pid[:, 1])):.3f} rad/s\")\n",
    "print(f\"  Total control effort: {np.sum(u_pid[:, 0]**2 + u_pid[:, 1]**2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cumulative cost\n",
    "def compute_cumulative_cost(states, controls, goals_list, Q, R, threshold=0.05):\n",
    "    \"\"\"\n",
    "    Compute cumulative LQR cost: sum(e'*Q*e + u'*R*u)\n",
    "    \"\"\"\n",
    "    cost = 0.0\n",
    "    goal_idx = 0\n",
    "    \n",
    "    for i in range(len(controls)):\n",
    "        state = states[i]\n",
    "        control = controls[i]\n",
    "        goal = goals_list[goal_idx]\n",
    "        \n",
    "        # Error\n",
    "        error = state - goal\n",
    "        error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "        \n",
    "        # Cost\n",
    "        cost += error @ Q @ error + control @ R @ control\n",
    "        \n",
    "        # Check goal switch\n",
    "        distance = np.linalg.norm(error[:2])\n",
    "        if distance < threshold and np.abs(error[2]) < 0.1 and goal_idx < len(goals_list) - 1:\n",
    "            goal_idx += 1\n",
    "    \n",
    "    return cost\n",
    "\n",
    "cost_lqr = compute_cumulative_cost(x_lqr, u_lqr, goals, Q, R)\n",
    "cost_pid = compute_cumulative_cost(x_pid, u_pid, goals, Q, R)\n",
    "\n",
    "print(\"\\n=== Performance Comparison ===\")\n",
    "print(f\"\\nCumulative LQR Cost:\")\n",
    "print(f\"  LQR: {cost_lqr:.2f}\")\n",
    "print(f\"  PID: {cost_pid:.2f}\")\n",
    "print(f\"  LQR improvement: {(cost_pid - cost_lqr) / cost_pid * 100:.1f}% lower cost\")\n",
    "\n",
    "# Tracking performance\n",
    "print(f\"\\nPosition Tracking Error:\")\n",
    "print(f\"  LQR: mean={np.mean(error_lqr):.4f} m, max={np.max(error_lqr):.4f} m\")\n",
    "print(f\"  PID: mean={np.mean(error_pid):.4f} m, max={np.max(error_pid):.4f} m\")\n",
    "\n",
    "# Settling time (time to reach within threshold of final goal)\n",
    "final_goal = goals[-1]\n",
    "settling_idx_lqr = np.where(error_lqr < goal_threshold)[0]\n",
    "settling_idx_pid = np.where(error_pid < goal_threshold)[0]\n",
    "\n",
    "if len(settling_idx_lqr) > 0:\n",
    "    settling_time_lqr = settling_idx_lqr[-1] * dt\n",
    "    print(f\"\\nSettling Time (to final goal):\")\n",
    "    print(f\"  LQR: {settling_time_lqr:.1f} s\")\n",
    "\n",
    "if len(settling_idx_pid) > 0:\n",
    "    settling_time_pid = settling_idx_pid[-1] * dt\n",
    "    print(f\"  PID: {settling_time_pid:.1f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with pykal's DynamicalSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykal import DynamicalSystem\n",
    "from pykal.algorithm_library.controllers.lqr import LQR\n",
    "\n",
    "# Create LQR controller as DynamicalSystem\n",
    "lqr_system = DynamicalSystem(\n",
    "    f=LQR.f,\n",
    "    state_name='state',\n",
    "    h=lambda u, **kwargs: u  # Control output\n",
    ")\n",
    "\n",
    "# Unicycle plant\n",
    "def unicycle_dynamics(state, u, dt, **kwargs):\n",
    "    \"\"\"Unicycle dynamics.\"\"\"\n",
    "    x, y, theta = state\n",
    "    v, omega = u\n",
    "    \n",
    "    x_next = x + v * np.cos(theta) * dt\n",
    "    y_next = y + v * np.sin(theta) * dt\n",
    "    theta_next = theta + omega * dt\n",
    "    theta_next = np.arctan2(np.sin(theta_next), np.cos(theta_next))\n",
    "    \n",
    "    return np.array([x_next, y_next, theta_next])\n",
    "\n",
    "unicycle_plant = DynamicalSystem(\n",
    "    f=unicycle_dynamics,\n",
    "    state_name='state',\n",
    "    h=lambda state, **kwargs: state\n",
    ")\n",
    "\n",
    "# Simulation with DynamicalSystem composition\n",
    "param_dict = {\n",
    "    'state': np.array([0.0, 0.0, 0.0]),\n",
    "    'goal': np.array([1.0, 1.0, np.pi/4]),\n",
    "    'K': K_lqr,\n",
    "    'dt': dt\n",
    "}\n",
    "\n",
    "print(\"\\n=== DynamicalSystem Integration Test ===\")\n",
    "for k in range(5):\n",
    "    # Compute control\n",
    "    error = param_dict['state'] - param_dict['goal']\n",
    "    error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "    u = -param_dict['K'] @ error\n",
    "    u = np.clip(u, [-V_MAX, -OMEGA_MAX], [V_MAX, OMEGA_MAX])\n",
    "    \n",
    "    param_dict['u'] = u\n",
    "    \n",
    "    # Update plant\n",
    "    next_state = unicycle_plant.step(params=param_dict)\n",
    "    \n",
    "    print(f\"Step {k}: state=[{next_state[0]:.3f}, {next_state[1]:.3f}, {next_state[2]:.3f}], \"\n",
    "          f\"control=[{u[0]:.3f}, {u[1]:.3f}]\")\n",
    "\n",
    "print(\"\\nâœ“ LQR successfully integrated with DynamicalSystem framework\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROS2 Deployment\n",
    "\n",
    "### Creating LQR Node for TurtleBot3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from pykal import ROSNode\n",
    "from nav_msgs.msg import Odometry\n",
    "from geometry_msgs.msg import Twist, PoseStamped\n",
    "\n",
    "# LQR callback for ROS2\n",
    "def lqr_callback(tk, odom, goal, **kwargs):\n",
    "    \"\"\"\n",
    "    LQR control callback for ROS2.\n",
    "    \n",
    "    Args:\n",
    "        tk: Current time (s)\n",
    "        odom: Odometry message [x, y, theta] from /odom topic\n",
    "        goal: Goal pose [x, y, theta] from /goal topic\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'cmd_vel' key -> Twist message\n",
    "    \"\"\"\n",
    "    # Extract state\n",
    "    state = np.array([odom[0], odom[1], odom[2]])  # [x, y, theta]\n",
    "    goal_state = np.array([goal[0], goal[1], goal[2]])\n",
    "    \n",
    "    # Compute error\n",
    "    error = state - goal_state\n",
    "    error[2] = np.arctan2(np.sin(error[2]), np.cos(error[2]))\n",
    "    \n",
    "    # LQR control\n",
    "    u = -kwargs['K'] @ error\n",
    "    \n",
    "    # Saturate\n",
    "    v = np.clip(u[0], -kwargs['v_max'], kwargs['v_max'])\n",
    "    omega = np.clip(u[1], -kwargs['omega_max'], kwargs['omega_max'])\n",
    "    \n",
    "    # Return as velocity command array [linear.x, linear.y, linear.z, angular.x, angular.y, angular.z]\n",
    "    cmd_vel = np.array([v, 0.0, 0.0, 0.0, 0.0, omega])\n",
    "    \n",
    "    return {'cmd_vel': cmd_vel}\n",
    "\n",
    "# Create ROS node (example - not executed in notebook)\n",
    "lqr_node_config = {\n",
    "    'callback': lqr_callback,\n",
    "    'subscriptions': [\n",
    "        ('/odom', Odometry, 'odom'),\n",
    "        ('/goal_pose', PoseStamped, 'goal')\n",
    "    ],\n",
    "    'publications': [\n",
    "        ('cmd_vel', Twist, '/cmd_vel')\n",
    "    ],\n",
    "    'param_dict': {\n",
    "        'K': K_lqr,\n",
    "        'v_max': V_MAX,\n",
    "        'omega_max': OMEGA_MAX\n",
    "    },\n",
    "    'node_name': 'turtlebot3_lqr_controller',\n",
    "    'rate': 10  # 10 Hz control rate\n",
    "}\n",
    "\n",
    "print(\"\\n=== ROS2 Node Configuration ===\")\n",
    "print(f\"Node name: {lqr_node_config['node_name']}\")\n",
    "print(f\"Control rate: {lqr_node_config['rate']} Hz\")\n",
    "print(f\"Subscriptions: {len(lqr_node_config['subscriptions'])} topics\")\n",
    "print(f\"Publications: {len(lqr_node_config['publications'])} topics\")\n",
    "print(\"\\nTo deploy:\")\n",
    "print(\"  1. lqr_node = ROSNode(**lqr_node_config)\")\n",
    "print(\"  2. lqr_node.create_node()\")\n",
    "print(\"  3. lqr_node.start()\")\n",
    "print(\"  4. # ... robot navigates to goals ...\")\n",
    "print(\"  5. lqr_node.stop()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Key Results\n",
    "\n",
    "This notebook demonstrated LQR control for TurtleBot3 point stabilization with the following findings:\n",
    "\n",
    "1. **Optimality**: LQR achieves lower cumulative cost compared to PID by balancing tracking error and control effort optimally\n",
    "\n",
    "2. **Smooth Control**: LQR generates smoother control signals due to the cost function penalizing control effort, reducing actuator wear\n",
    "\n",
    "3. **Guaranteed Stability**: For linear systems, LQR guarantees closed-loop stability with the computed feedback gain\n",
    "\n",
    "4. **Systematic Tuning**: The Q and R matrices provide intuitive tuning parameters (state vs control weighting) compared to PID's three independent gains per axis\n",
    "\n",
    "5. **Performance Trade-off**: LQR typically achieves better performance than PID in terms of the LQR cost function, though PID may converge faster in some cases\n",
    "\n",
    "### When to Use LQR\n",
    "\n",
    "**Use LQR when:**\n",
    "- System dynamics can be reasonably linearized\n",
    "- You want to optimize a specific cost function (tracking + effort)\n",
    "- Smooth control signals are important (battery life, actuator wear)\n",
    "- Guaranteed stability is required\n",
    "- Systematic tuning is preferred over trial-and-error\n",
    "\n",
    "**Use PID when:**\n",
    "- System model is unknown or highly uncertain\n",
    "- Simple implementation is critical\n",
    "- Aggressive response is more important than optimality\n",
    "- Working with legacy systems already tuned for PID\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **Mobile Robot Navigation**: Warehouse robots, delivery robots\n",
    "- **Trajectory Tracking**: Following paths in structured environments\n",
    "- **Formation Control**: Multi-robot coordination\n",
    "- **Docking**: Precision positioning for charging stations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Time-Varying LQR**: Handle trajectory tracking (not just point stabilization)\n",
    "2. **Adaptive LQR**: Update gains online based on system identification\n",
    "3. **Robust LQR**: Add robustness to model uncertainties\n",
    "4. **Hardware Testing**: Deploy on physical TurtleBot3 with Gazebo validation\n",
    "5. **Obstacle Avoidance**: Combine with path planning algorithms\n",
    "\n",
    "### References\n",
    "\n",
    "For the theoretical foundation and implementation details, see:\n",
    "- LQR theory: :cite:`anderson2007optimal`\n",
    "- Mobile robot control: :cite:`siegwart2011introduction`\n",
    "- TurtleBot3 documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
